# lab

echo "source <(kubectl completion bash)" >> ~/.bashrc
source ~/.bashrc

mkdir .kube  # create the .kube directory
master_ip=$(aws ec2 describe-instances --region us-west-2 \
 --filters "Name=tag:Name,Values=k8s-master" \
 --query "Reservations[*].Instances[*].PrivateIpAddress" \
 --output text)  # get the master's IP address using the AWS CLI
scp -o "ForwardAgent yes" $master_ip:.kube/config .kube/config  # secure copy (scp) the kubeconfig file

cat ~/.kube/config

kubectl config get-contexts

kubectl auth can-i list nodes

kubectl get clusterrolebindings

kubectl get clusterrole cluster-admin -o yaml
kubectl describe clusterrole cluster-admin

kubectl describe clusterrolebinding cluster-admin

# Extract the kubernetes-admin certificate from the kubeconfig file, 
# and use OpenSSL to show the certificate details

grep "client-cert" ~/.kube/config | \
  sed 's/\(.*client-certificate-data: \)\(.*\)/\2/' | \
  base64 --decode \
  > cert.pem
openssl x509 -in cert.pem -text -noout

# Subject: O=system:masters, CN=kubernetes-admin

You now understand the full authorization chain:
- The context uses a kubernetes-admin client certificate
- The certificate declares the kubernetes admin user as a member of the system:masters organization, 
  which is treated as a group in Kubernetes
- There is a cluster role binding between the system:masters group and the cluster-admin cluster role
- The cluster-admin cluster role grants all actions on all resources

# kubectl delete node <name_of_node>

kubectl get nodes -o wide

worker_dns=$(kubectl get nodes | grep \<none\> | cut -d' ' -f1 | head -1)
ssh $worker_dns

sudo systemctl status kubelet

journalctl -u kubelet

sudo systemctl stop kubelet

# you should check the status of both the kubelet and the container runtime

sudo systemctl status kubelet

kubectl get nodes

ssh $worker_dns
sudo systemctl start kubelet
exit

# Connect to the worker node and restart its kubelet with an 
# additional option to make it detect memory pressure

ssh $worker_dns
sudo sed -i 's%\(/usr/bin/kubelet\) %\1 --eviction-hard=memory.available<0.95Gi %' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
sudo systemctl daemon-reload  # reload the modified configuration file
sudo systemctl restart kubelet
exit

kubectl get nodes
kubectl describe node $worker_dns | more

# MemoryPressure, OutOfDisk, DiskPressure, PIDPressure
# drain or cordon the node to prevent pods from being scheduled on the node
# uncordon the node to allow pods to be scheduled on the node again

# NodeHasInsufficientMemory, EvictionThresholdMet

ssh $worker_dns
sudo sed -i 's%--eviction-hard=memory.available<0.95Gi %%' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
sudo systemctl daemon-reload
sudo systemctl restart kubelet
exit

kubectl describe node $worker_dns

Recall that every node in a Kubernetes cluster has the following components:

kubelet: The primary node agent that accepts pod specifications
Container runtime: Software responsible for running containers, for example, Docker
kube-proxy: Implements network rules and connection forwarding to enable the Kubernetes service abstraction 

master nodes also include the following components to provide the Kubernetes control plane:

kube-apiserver: Exposes the Kubernetes API
etcd: Key-value store for backing cluster data
kube-scheduler: Responsible for scheduling pods onto nodes
kube-controller-manager: Responsible for running Kubernetes controllers, 
  for example, the node controller that responds to changes in a node's status

kubectl get pods -n kube-system
kubectl get daemonset -n kube-system
kubectl get pods -n kube-system -o wide

kubectl get daemonset kube-proxy -n kube-system -o yaml --export | more

# proxy_pod_1 is a variable with the name of the first kube-proxy pod
proxy_pod_1=$(kubectl get pods -n kube-system | grep proxy | cut -d" " -f1 | head -1)
kubectl logs -n kube-system $proxy_pod_1

kubectl delete pod $proxy_pod_1 -n kube-system
kubectl get daemonset -n kube-system 

ssh $master_ip
# Get the name of the kube-apiserver pod
apiserver_pod=$(kubectl get pods -n kube-system | grep apiserver | cut -d" " -f1)
# change the pod's image to hello-world using the patch command
kubectl patch pod $apiserver_pod -n kube-system \
  -p '{"spec":{"containers":[{"name":"kube-apiserver","image":"hello-world"}]}}'

kubectl describe pod $apiserver_pod -n kube-system | more

# Static pods are managed directly by the node's kubelet and not by the API server

# The --pod-manifest-path option of the kubelet sets the directory of the static pod specifications, 
# in this case, /etc/kubernetes/manifests.
# kubelet -h | grep pod-manifests

ls /etc/kubernetes/manifests
sudo more /etc/kubernetes/manifests/etcd.yaml
ss -tl

# the example shows you how to use etcdctl 
etcd_pod=$(kubectl get pods -n kube-system | grep ^etcd | cut -d" " -f1)
kubectl exec -n kube-system $etcd_pod -- \
  /bin/sh -ec \
  'ETCDCTL_API=3 \
   etcdctl \
   --endpoints=127.0.0.1:2379 \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt \
   --cert=/etc/kubernetes/pki/etcd/peer.crt \
   --key=/etc/kubernetes/pki/etcd/peer.key \
   get \
   /registry/clusterrolebindings/cluster-admin'

kubectl get componentstatus
kubectl cluster-info --help

wget -qO /tmp/metrics-server.zip https://github.com/cloudacademy/metrics-server/archive/master.zip
sudo apt install unzip && \
  unzip -q -d /tmp /tmp/metrics-server.zip && \
  kubectl create -f /tmp/metrics-server-master/deploy/1.8+/

cat > app.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: legacy
spec:
  containers:
  - name: loop
    image: alpine:3.7
    command:
    - /bin/sh 
    - -ec
    - while true; do echo hi >> /logs; sleep 2; done
EOF

kubectl create -f app.yaml

kubectl logs legacy loop

cat > load.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: load
spec:
  containers:
  - name: cpu-load
    image: vish/stress
    args:
    - -cpus
    - "2"
EOF

kubectl create -f load.yaml

cat > load-limited.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: load-limited
spec:
  containers:
  - name: cpu-load-limited
    image: vish/stress
    args:
    - -cpus
    - "2"
    resources:
      limits:
        cpu: "0.5"
      requests:
        cpu: "0.5"
EOF

kubectl create -f load-limited.yaml