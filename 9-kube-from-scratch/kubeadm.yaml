# lab

# Update the package index
sudo apt-get update
# Update packages required for HTTPS package repository access
sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common

# Install Docker community edition using Ubuntu's apt package manager 
# and the official Docker repository

# Add Dockerâ€™s GPG key
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
# Configure the stable Docker release repository
sudo add-apt-repository \
 "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
 $(lsb_release -cs) \
 stable"
# Update the package index to include the stable Docker repository
sudo apt-get update
# Install Docker
sudo apt-get install -y docker-ce=17.03.2~ce-0~ubuntu-xenial

docker --version
# Docker version 17.03.2-ce, build f5ec1e2

# Install kubeadm, kubectl, and kubelet from the official Kubernetes package repository
# Add the Google Cloud packages GPG key
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
# Add the Kubernetes release repository
sudo add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
# Update the package index to include the Kubernetes repository
sudo apt-get update
# Install the packages
sudo apt-get install -y kubeadm=1.13.4-00 kubelet=1.13.4-00 kubectl=1.13.4-00 kubernetes-cni=0.6.0-00
# Prevent automatic updates to the installed packages
sudo apt-mark hold kubelet kubeadm kubectl

# The version of all the packages is set to 1.13.4 for consistency
# perform a cluster upgrade in a later

kubeadm

# You will use Calico as the pod network plugin. Calico supports Kubernetes network policies.
# For network policies to function properly, 
# you must use the --pod-network-cidr option to specify a range of IP addresses 
# for the pod network when initializing the master node with kubeadm. 

# Initialize the master node using the init command
# The pod network CIDR block (192.168.0.0/16) is the default used by Calico.
sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --kubernetes-version=stable-1.13

### Output at the end:
Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 10.0.0.23:6443 --token bwzckv.qf26bt8v9al86kvb --discovery-token-ca-cert-hash sha256:f9b3abe17de096bb46b4373885a26bb873bf096a1f840e19c1093ff12d4b3a8f
###

# The join tokens expire after 24 hours by default.
# create new tokens using the kubeadm token command if required

# Initialize your user's default kubectl configuration using 
# the admin kubeconfig file generated by kubeadm
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get componentstatuses

# kubeadm token --help

kubectl get nodes
kubectl describe nodes

# The cni config uninitialized issue
# resolve the issue by initializing the Calico network plugin

kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml

# A daemonset is used to run a calico-node pod on each node in the cluster. 

kubectl get nodes

# SSH into another EC2 instance
sudo kubeadm join 10.0.0.23:6443 --token bwzckv.qf26bt8v9al86kvb --discovery-token-ca-cert-hash sha256:f9b3abe17de096bb46b4373885a26bb873bf096a1f840e19c1093ff12d4b3a8f

# Output: This node has joined the cluster

kubectl get nodes
kubectl get pods --all-namespaces

# Backing Up and Restoring Kubernetes Clusters
# The state information of a Kubernetes cluster is stored in etcd.
# you should also back up the cluster's certificate authority key (/etc/kubernetes/pki/ca.key) 
# and certificate (/etc/kubernetes/pki/ca.crt)

# Create a deployment of the Nginx application with two replicas
kubectl create deployment nginx --image=nginx
kubectl scale deployment nginx --replicas=2

kubectl expose deployment nginx --name=clusterip --port=80 --target-port=80 --name=web

# Send a HTTP request to the web service
# Get the Cluster IP of the service
service_ip=$(kubectl get service web -o jsonpath='{.spec.clusterIP}')
# Use curl to send an HTTP request to the service
curl $service_ip

kubectl create namespace management 

# Create a job that creates a pod, and issues the etcdctl snapshot save command to back up the cluster
cat <<EOF | kubectl create -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: backup
  namespace: management
spec:
  template:
    spec:
      containers:
      # Use etcdctl snapshot save to create a snapshot in the /snapshot directory 
      - command:
        - /bin/sh 
        args:
        - -ec
        - etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key snapshot save /snapshots/backup.db
        # The same image used by the etcd pod
        image: k8s.gcr.io/etcd-amd64:3.1.12
        name: etcdctl
        env:
        # Set the etcdctl API version to 3 (to match the version of etcd installed by kubeadm)
        - name: ETCDCTL_API
          value: '3'
        volumeMounts:
        - mountPath: /etc/kubernetes/pki/etcd
          name: etcd-certs
          readOnly: true
        - mountPath: /snapshots
          name: snapshots
      # Use the host network where the etcd port is accessible (etcd pod uses host network)
      # This allows the etcdctl to connect to etcd that is listening on the host network
      hostNetwork: true
      affinity:
        # Use node affinity to schedule the pod on the master (where the etcd pod is)
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: Exists
      restartPolicy: OnFailure
      tolerations:
      # tolerate the master's NoSchedule taint to allow scheduling on the master
      - effect: NoSchedule
        operator: Exists
      volumes:
      # Volume storing the etcd PKI keys and certificates
      - hostPath:
          path: /etc/kubernetes/pki/etcd
          type: DirectoryOrCreate
        name: etcd-certs
      # A volume to store the backup snapshot
      - hostPath:
          path: /snapshots
          type: DirectoryOrCreate
        name: snapshots
EOF

# You could also create a CronJob Kubernetes resource instead of a one-off Job
ls /snapshots

# Simulate the backup and restore
# Stop the master's kubelet
sudo systemctl stop kubelet.service

# Delete the etcd containers in Docker that are created by the Kubernetes etcd pod
sudo docker ps | grep etcd | cut -d' ' -f1 | xargs sudo docker rm -f

# The etcd pod mounts /var/lib/etcd to persist its data to disk
# Delete the etcd data files persisted to disk
sudo rm -rf /var/lib/etcd/*

# Use a Docker container to restore the /var/lib/etcd data from the backup snapshot
sudo docker run --rm \
    -v '/snapshots:/snapshots' \
    -v '/var/lib/etcd:/var/lib/etcd' \
    -e ETCDCTL_API=3 \
    'k8s.gcr.io/etcd-amd64:3.1.12' \
    /bin/sh -c "etcdctl snapshot restore '/snapshots/backup.db' && mv /default.etcd/member /var/lib/etcd"
  
# The kubelet will recreate the etcd pod from the static pod manifest 
# in /etc/kubernetes/manifests/etcd.yaml. 
# The etcdctl snapshot restore command performs the restore operation

sudo systemctl start kubelet

kubectl get pods

service_ip=$(kubectl get service web -o jsonpath='{.spec.clusterIP}')
curl $service_ip

# Upgrading Kubernetes Clusters with kubeadm
# upgrading Kubernetes from version 1.10.6 to version 1.11.1

The upgrade process follows the general procedure of:

- Upgrading the Kubernetes control plane with kubeadm (Kubernetes components and add-ons excluding the CNI)
- Manually upgrading the CNI network plugin, if applicable (For this Lab, the installed version 3.1 of Calico is already the appropriate one for Kubernetes version 1.11.1)
- Upgrading the Kubernetes packages (kubelet, kubeadm, kubectl) on the master and worker nodes
- Upgrading the kubelet config on worker nodes with kubeadm

# Download version 1.14.1 of kubeadm
# Update the kubeadm binary with version 1.14.1
sudo curl -sSL https://dl.k8s.io/release/v1.14.1/bin/linux/amd64/kubeadm -o /usr/bin/kubeadm

# Currently, you cannot upgrade kubeadm using the apt package manager until after upgrading the control plane. 
# This limitation should be removed in future versions of kubeadm.

# Generate an upgrade plan for upgrading Kubernetes to version 1.14.1
ubuntu@ip-10-0-0-23:~$ sudo kubeadm upgrade plan v1.14.1
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.13.12
[upgrade/versions] kubeadm version: v1.14.1

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       AVAILABLE
Kubelet     2 x v1.13.4   v1.14.1

Upgrade to the latest version in the v1.13 series:

COMPONENT            CURRENT    AVAILABLE
API Server           v1.13.12   v1.14.1
Controller Manager   v1.13.12   v1.14.1
Scheduler            v1.13.12   v1.14.1
Kube Proxy           v1.13.12   v1.14.1
CoreDNS              1.2.6      1.3.1
Etcd                 3.2.24     3.3.10

You can now apply the upgrade by executing the following command:

	kubeadm upgrade apply v1.14.1

_____________________________________________________________________

# Apply the upgrade plan by issuing the following command and entering y when prompted
sudo kubeadm upgrade apply v1.14.1

Note: 
  - If the upgrade procedure times out, you can safely try again until it succeeds. 
  - The upgrade command is idempotent so you can run the command as many times as required 
  - to complete the upgrade. 
  - This issue is being worked on and should be resolved in future versions of kubeadm.

# Prepare to upgrade the master node's kubelet by draining the node
kubectl drain $HOSTNAME --ignore-daemonsets
# node/ip-10-0-0-23 cordoned ...

# Upgrade the kubelet, kubeadm, and kubectl apt packages
sudo apt-get update
sudo apt-get upgrade -y --allow-change-held-packages \
     kubelet=1.14.1-00 kubeadm=1.14.1-00 kubectl=1.14.1-00
    
Note: 
  - If you see a Configuring grub-pc menu, select Keep the local version currently installed
  - Press space to select the first device, followed by enter to press the <Ok> button

# Uncordon the master to allow pods to be scheduled on it now that is has been upgraded
kubectl uncordon $HOSTNAME

# Get the node information to confirm that the version of the master is 1.14.1
kubectl get nodes

# Drain the worker node to prepare it for upgrading
# Get the worker's name
worker_name=$(kubectl get nodes | grep \<none\> | cut -d' ' -f1)
# Drain the worker node
kubectl drain $worker_name --ignore-daemonsets

# In the worker node
sudo apt-get update
sudo apt-get upgrade -y --allow-change-held-packages \
     kubelet=1.14.1-00 kubeadm=1.14.1-00 kubectl=1.14.1-00
    
# Upgrade the worker node's kubelet config using kubeadm
sudo kubeadm upgrade node config --kubelet-version v1.14.1

sudo systemctl restart kubelet

# from master node
kubectl uncordon $worker_name

kubectl get nodes

In this Lab Step, you went through the process of upgrading the cluster using kubeadm.

This brings you to the end of the guided portion of the Lab. Before ending, there are a few points to make regarding considerations for production clusters:

All of the instances in the Lab were in a public subnet. This was to allow to focus more on creating the cluster and not AWS-specific network security. In practice, you would use a private subnet for the cluster nodes, and use a load balancer in a public subnet to access the API server and a bastion host in a public subnet to connect to the instances via SSH.
All of the instances shared the same instance role and have the same permissions. In practice, the worker nodes do not need as many privileges as a master node. The master needs to be able to create load balancers for load balanced services and volumes for persistent volumes. Worker nodes do not. As a best practice, using the least amount of privilege possible for each type of node will improve security of your cluster.
In cloud environments, you can use auto scaling capabilities to automatically add worker nodes to your cluster when certain conditions are met, such as all nodes using a lot of CPU.
You would want a highly-available control plane in production. At least you would want to have monitoring in place to automatically restart the master if it fails, but running a multi-master cluster will prevent downtime and protect you from zone outages.

